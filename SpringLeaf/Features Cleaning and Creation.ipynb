{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Springleaf Marketing Response\n",
    "\n",
    "## Features Creation and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>VAR_0001</th>\n",
       "      <th>VAR_0002</th>\n",
       "      <th>VAR_0003</th>\n",
       "      <th>VAR_0004</th>\n",
       "      <th>VAR_0005</th>\n",
       "      <th>VAR_0006</th>\n",
       "      <th>VAR_0007</th>\n",
       "      <th>VAR_0008</th>\n",
       "      <th>VAR_0009</th>\n",
       "      <th>...</th>\n",
       "      <th>VAR_1926</th>\n",
       "      <th>VAR_1927</th>\n",
       "      <th>VAR_1928</th>\n",
       "      <th>VAR_1929</th>\n",
       "      <th>VAR_1930</th>\n",
       "      <th>VAR_1931</th>\n",
       "      <th>VAR_1932</th>\n",
       "      <th>VAR_1933</th>\n",
       "      <th>VAR_1934</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>H</td>\n",
       "      <td>224</td>\n",
       "      <td>0</td>\n",
       "      <td>4300</td>\n",
       "      <td>C</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>false</td>\n",
       "      <td>false</td>\n",
       "      <td>...</td>\n",
       "      <td>98</td>\n",
       "      <td>98</td>\n",
       "      <td>998</td>\n",
       "      <td>999999998</td>\n",
       "      <td>998</td>\n",
       "      <td>998</td>\n",
       "      <td>9998</td>\n",
       "      <td>9998</td>\n",
       "      <td>IAPS</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>H</td>\n",
       "      <td>7</td>\n",
       "      <td>53</td>\n",
       "      <td>4448</td>\n",
       "      <td>B</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>false</td>\n",
       "      <td>false</td>\n",
       "      <td>...</td>\n",
       "      <td>98</td>\n",
       "      <td>98</td>\n",
       "      <td>998</td>\n",
       "      <td>999999998</td>\n",
       "      <td>998</td>\n",
       "      <td>998</td>\n",
       "      <td>9998</td>\n",
       "      <td>9998</td>\n",
       "      <td>IAPS</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>H</td>\n",
       "      <td>116</td>\n",
       "      <td>3</td>\n",
       "      <td>3464</td>\n",
       "      <td>C</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>false</td>\n",
       "      <td>false</td>\n",
       "      <td>...</td>\n",
       "      <td>98</td>\n",
       "      <td>98</td>\n",
       "      <td>998</td>\n",
       "      <td>999999998</td>\n",
       "      <td>998</td>\n",
       "      <td>998</td>\n",
       "      <td>9998</td>\n",
       "      <td>9998</td>\n",
       "      <td>IAPS</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>H</td>\n",
       "      <td>240</td>\n",
       "      <td>300</td>\n",
       "      <td>3200</td>\n",
       "      <td>C</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>false</td>\n",
       "      <td>false</td>\n",
       "      <td>...</td>\n",
       "      <td>98</td>\n",
       "      <td>98</td>\n",
       "      <td>998</td>\n",
       "      <td>999999998</td>\n",
       "      <td>998</td>\n",
       "      <td>998</td>\n",
       "      <td>9998</td>\n",
       "      <td>9998</td>\n",
       "      <td>RCC</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>R</td>\n",
       "      <td>72</td>\n",
       "      <td>261</td>\n",
       "      <td>2000</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>false</td>\n",
       "      <td>false</td>\n",
       "      <td>...</td>\n",
       "      <td>98</td>\n",
       "      <td>98</td>\n",
       "      <td>998</td>\n",
       "      <td>999999998</td>\n",
       "      <td>998</td>\n",
       "      <td>998</td>\n",
       "      <td>9998</td>\n",
       "      <td>9998</td>\n",
       "      <td>BRANCH</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 1934 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID VAR_0001  VAR_0002  VAR_0003  VAR_0004 VAR_0005  VAR_0006  VAR_0007  \\\n",
       "0   2        H       224         0      4300        C         0         0   \n",
       "1   4        H         7        53      4448        B         1         0   \n",
       "2   5        H       116         3      3464        C         0         0   \n",
       "3   7        H       240       300      3200        C         0         0   \n",
       "4   8        R        72       261      2000        N         0         0   \n",
       "\n",
       "  VAR_0008 VAR_0009   ...   VAR_1926 VAR_1927 VAR_1928   VAR_1929  VAR_1930  \\\n",
       "0    false    false   ...         98       98      998  999999998       998   \n",
       "1    false    false   ...         98       98      998  999999998       998   \n",
       "2    false    false   ...         98       98      998  999999998       998   \n",
       "3    false    false   ...         98       98      998  999999998       998   \n",
       "4    false    false   ...         98       98      998  999999998       998   \n",
       "\n",
       "   VAR_1931  VAR_1932  VAR_1933  VAR_1934  target  \n",
       "0       998      9998      9998      IAPS       0  \n",
       "1       998      9998      9998      IAPS       0  \n",
       "2       998      9998      9998      IAPS       0  \n",
       "3       998      9998      9998       RCC       0  \n",
       "4       998      9998      9998    BRANCH       1  \n",
       "\n",
       "[5 rows x 1934 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtypeSpecificator = {'VAR_0008':object,'VAR_0009':object,'VAR_0010':object,\n",
    "                     'VAR_0011':object,'VAR_0012':object,'VAR_0043':object,\n",
    "                     'VAR_0157':object,'VAR_0167':object,'VAR_0177':object,\n",
    "                     'VAR_0196':object,'VAR_0214':object,'VAR_0226':object,\n",
    "                     'VAR_0229':object,'VAR_0230':object,'VAR_0232':object,\n",
    "                     'VAR_0236':object,'VAR_0239':object }\n",
    "\n",
    "dataTrain = pd.read_csv(\"Data/train.csv\", dtype = dtypeSpecificator)\n",
    "dataTest = pd.read_csv(\"Data/test.csv\", dtype = dtypeSpecificator)\n",
    "x_data = dataTrain.drop(['ID','target'],axis=1)\n",
    "x_data_t = dataTest.drop(['ID'],axis=1)\n",
    "y_train = dataTrain['target']\n",
    "x_data.fillna(-1,inplace=True)\n",
    "x_data_t.fillna(-1,inplace=True)\n",
    "dataTrain.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw Data Cleaning\n",
    "- Find type of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Data type extraction\n",
    "\n",
    "# Find type for each columns.\n",
    "int_data = []\n",
    "float_data = []\n",
    "object_data = []\n",
    "\n",
    "for var in x_data.columns.values:\n",
    "    if x_data[var].dtype == 'object':\n",
    "        object_data.append(var)\n",
    "    elif  x_data[var].dtype == 'int64':\n",
    "        int_data.append(var)\n",
    "    elif x_data[var].dtype == 'float64':\n",
    "        float_data.append(var)\n",
    "            \n",
    "\n",
    "# Find number of unique values for each columns.\n",
    "out_data = []\n",
    "low_data = []\n",
    "low2_data = []\n",
    "high_data = []\n",
    "in_out_data = []\n",
    "\n",
    "for var in x_data.columns.values:\n",
    "    if len(x_data[var].unique()) == 1:\n",
    "        out_data.append(var)\n",
    "    elif len(x_data[var].unique()) == 2:\n",
    "        in_out_data.append(var)\n",
    "    elif  len(x_data[var].unique()) < 20:\n",
    "        low_data.append(var)\n",
    "    elif len(x_data[var].unique()) < 80:\n",
    "        low2_data.append(var)\n",
    "    else:\n",
    "        high_data.append(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#remove irrelevant data\n",
    "x_data.drop(out_data,axis=1,inplace=True)\n",
    "\n",
    "duplicate = {}\n",
    "dups = []\n",
    "\n",
    "#remove duplicates\n",
    "for sub_data in [in_out_data,low_data,low2_data,high_data]:\n",
    "    for i in range(len(sub_data)):\n",
    "        var1 = sub_data[i]\n",
    "        if var1 not in dups:\n",
    "            duplicate[var1] = []\n",
    "            for j in range(len(sub_data) - i - 1):\n",
    "                var2 =  sub_data[i+j+1]\n",
    "                if var2 not in dups:\n",
    "                    if len(x_data[var1].unique()) == len(x_data[var2].unique()):\n",
    "                        if  (x_data[var1] == x_data[var2]).all():\n",
    "                            duplicate[var1].append(var2)\n",
    "                            dups.append(var2)\n",
    "                            \n",
    "x_data.drop(dups,axis=1,inplace=True)\n",
    "x_data_t.drop(dups,axis=1,inplace=True)\n",
    "print len(dups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine Object columns data to treat them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of object data :  51\n",
      "number of int data :  1404\n",
      "number of float data :  477\n",
      "number of columns :  1878\n"
     ]
    }
   ],
   "source": [
    "print \"number of object data : \", len(object_data)\n",
    "print \"number of int data : \", len(int_data)\n",
    "print \"number of float data : \", len(float_data)\n",
    "print \"number of columns : \", x_data.shape[1]\n",
    "\n",
    "date_col = [\"VAR_0073\",\"VAR_0075\",\"VAR_0156\",\"VAR_0157\",\"VAR_0158\",\"VAR_0159\",\"VAR_0166\",\"VAR_0167\",\"VAR_0168\",\"VAR_0169\",\"VAR_0176\",\"VAR_0177\",\"VAR_0178\",\"VAR_0179\",\"VAR_0204\",\"VAR_0217\"]\n",
    "\n",
    "city_name = [\"VAR_0200\"]\n",
    "\n",
    "state_name = [\"VAR_0237\",\"VAR_0274\"]\n",
    "\n",
    "employement_data = [\"VAR_0404\", \"VAR_0493\"]\n",
    "\n",
    "year_col = [\"VAR_0294\",\"VAR_0314\",\"VAR_0332\",\"VAR_0531\"]\n",
    "\n",
    "#for var in high_data[100:120]:\n",
    "#    if var not in object_data :\n",
    "#        k = len(x_data[var].unique())*10\n",
    "#        print var, k/10\n",
    "#        plt.plot(x_data[y_train == 0][var].value_counts())\n",
    "#        plt.plot(x_data[y_train == 1][var].value_counts())\n",
    "#        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treat Data \n",
    "\n",
    "First convert date_col, year_col into time stamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Treat date\n",
    "import time \n",
    "\n",
    "dateFormat = \"%d%b%y:%H:%M:%S\"\n",
    "yearFormat = \"%Y\"\n",
    "yearmonthFormat = \"%Y%m\"\n",
    "\n",
    "for var in date_col:\n",
    "    x_data[var] =  x_data[var].apply(lambda x : time.mktime(time.strptime(x, dateFormat)) if x != -1 else x)\n",
    "    x_data_t[var] =  x_data_t[var].apply(lambda x : time.mktime(time.strptime(x, dateFormat)) if x != -1 else x)\n",
    "    \n",
    "for var in year_col[:-1]:\n",
    "    x_data[var] = x_data[var].apply(lambda x : time.mktime(time.strptime(str(int(x)), yearFormat)) if x != -1 else x)\n",
    "    x_data_t[var] = x_data_t[var].apply(lambda x : time.mktime(time.strptime(str(int(x)), yearFormat)) if x != -1 else x)\n",
    "    \n",
    "for var in year_col[-1:]:\n",
    "    x_data[var] = x_data[var].apply(lambda x : time.mktime(time.strptime(str(int(x)), yearmonthFormat)) if x != -1 else x)\n",
    "    x_data_t[var] = x_data_t[var].apply(lambda x : time.mktime(time.strptime(str(int(x)), yearmonthFormat)) if x != -1 else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert employement data using CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<145231x43 sparse matrix of type '<type 'numpy.int64'>'\n",
       " \twith 16817 stored elements in Compressed Sparse Row format>,\n",
       " <145231x63 sparse matrix of type '<type 'numpy.int64'>'\n",
       " \twith 18268 stored elements in Compressed Sparse Row format>]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Treat employement data\n",
    "\n",
    "\n",
    "stopwords ='and of a or in on'.split(' ')\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "counts = []\n",
    "dfs = []\n",
    "dfst = []\n",
    "for var in employement_data:\n",
    "    count = CountVectorizer(min_df=50,binary=True,stop_words=stopwords)\n",
    "    x_data[var] = x_data[var].apply(lambda x: \"undetermined\" if x == -1 else x)\n",
    "    x_data_t[var] = x_data_t[var].apply(lambda x: \"undetermined\" if x == -1 else x)\n",
    "    test = count.fit_transform(x_data[var])\n",
    "    test2 = count.transform(x_data_t[var])\n",
    "    counts.append(count)\n",
    "    dfs.append(test)\n",
    "    dfst.append(test2)\n",
    "dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert city data using CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<145231x96 sparse matrix of type '<type 'numpy.int64'>'\n",
       " \twith 38283 stored elements in Compressed Sparse Row format>]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#treat city data\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "countsc = []\n",
    "dfsc = []\n",
    "dfsct = [] \n",
    "for var in city_name:\n",
    "    count = CountVectorizer(min_df=200,binary=True)\n",
    "    x_data[var] = x_data[var].apply(lambda x: \"undetermined\" if x == -1 else x.replace(' ','_'))\n",
    "    x_data_t[var] = x_data_t[var].apply(lambda x: \"undetermined\" if x == -1 else x.replace(' ','_'))\n",
    "    test = count.fit_transform(x_data[var])\n",
    "    test2 = count.transform(x_data_t[var])\n",
    "    countsc.append(count)\n",
    "    dfsc.append(test)\n",
    "    dfsct.append(test2)\n",
    "dfsc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use sklearn to shatter categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(145231, 252)\n"
     ]
    }
   ],
   "source": [
    "categorical_data = []\n",
    "for var in object_data:\n",
    "    if var not in date_col + city_name + employement_data + dups:\n",
    "        categorical_data.append(var)\n",
    "\n",
    "########## eclatement\n",
    "from sklearn.feature_extraction import DictVectorizer as DV\n",
    "dat_dict = x_data[categorical_data].T.to_dict().values()\n",
    "dat_dict2 = x_data_t[categorical_data].T.to_dict().values()\n",
    "vectorizer = DV(sparse = False)\n",
    "vectorizer.fit(dat_dict)\n",
    "dat = vectorizer.transform(dat_dict)\n",
    "dat2 = vectorizer.transform(dat_dict2)\n",
    "x_cat = pd.DataFrame(dat)\n",
    "x_cat_t = pd.DataFrame(dat2)\n",
    "print x_cat.shape\n",
    "\n",
    "#x_cat2 = x_data[categorical_data].copy()\n",
    "#\n",
    "########## creation de label dans une colonne unique\n",
    "#from sklearn.preprocessing import LabelEncoder\n",
    "#for var in categorical_data:\n",
    "#    le = LabelEncoder()\n",
    "#    le.fit(x_data[var])\n",
    "#    x_cat2[var] = le.transform(x_data[var])\n",
    "#\n",
    "#print x_cat2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create New Features from existing columns\n",
    "time related columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['uncompleted_date', 'delta_time_0_1', 'delta_time_0_2', 'delta_time_0_3', 'delta_time_0_4', 'delta_time_0_5', 'delta_time_0_6', 'delta_time_0_7', 'delta_time_0_8', 'delta_time_0_9', 'delta_time_0_10', 'delta_time_0_11', 'delta_time_0_12', 'delta_time_0_13', 'delta_time_0_14', 'delta_time_0_15', 'delta_time_1_2', 'delta_time_1_3', 'delta_time_1_4', 'delta_time_1_5', 'delta_time_1_6', 'delta_time_1_7', 'delta_time_1_8', 'delta_time_1_9', 'delta_time_1_10', 'delta_time_1_11', 'delta_time_1_12', 'delta_time_1_13', 'delta_time_1_14', 'delta_time_1_15', 'delta_time_2_3', 'delta_time_2_4', 'delta_time_2_5', 'delta_time_2_6', 'delta_time_2_7', 'delta_time_2_8', 'delta_time_2_9', 'delta_time_2_10', 'delta_time_2_11', 'delta_time_2_12', 'delta_time_2_13', 'delta_time_2_14', 'delta_time_2_15', 'delta_time_3_4', 'delta_time_3_5', 'delta_time_3_6', 'delta_time_3_7', 'delta_time_3_8', 'delta_time_3_9', 'delta_time_3_10', 'delta_time_3_11', 'delta_time_3_12', 'delta_time_3_13', 'delta_time_3_14', 'delta_time_3_15', 'delta_time_4_5', 'delta_time_4_6', 'delta_time_4_7', 'delta_time_4_8', 'delta_time_4_9', 'delta_time_4_10', 'delta_time_4_11', 'delta_time_4_12', 'delta_time_4_13', 'delta_time_4_14', 'delta_time_4_15', 'delta_time_5_6', 'delta_time_5_7', 'delta_time_5_8', 'delta_time_5_9', 'delta_time_5_10', 'delta_time_5_11', 'delta_time_5_12', 'delta_time_5_13', 'delta_time_5_14', 'delta_time_5_15', 'delta_time_6_7', 'delta_time_6_8', 'delta_time_6_9', 'delta_time_6_10', 'delta_time_6_11', 'delta_time_6_12', 'delta_time_6_13', 'delta_time_6_14', 'delta_time_6_15', 'delta_time_7_8', 'delta_time_7_9', 'delta_time_7_10', 'delta_time_7_11', 'delta_time_7_12', 'delta_time_7_13', 'delta_time_7_14', 'delta_time_7_15', 'delta_time_8_9', 'delta_time_8_10', 'delta_time_8_11', 'delta_time_8_12', 'delta_time_8_13', 'delta_time_8_14', 'delta_time_8_15', 'delta_time_9_10', 'delta_time_9_11', 'delta_time_9_12', 'delta_time_9_13', 'delta_time_9_14', 'delta_time_9_15', 'delta_time_10_11', 'delta_time_10_12', 'delta_time_10_13', 'delta_time_10_14', 'delta_time_10_15', 'delta_time_11_12', 'delta_time_11_13', 'delta_time_11_14', 'delta_time_11_15', 'delta_time_12_13', 'delta_time_12_14', 'delta_time_12_15', 'delta_time_13_14', 'delta_time_13_15', 'delta_time_14_15']\n"
     ]
    }
   ],
   "source": [
    "time_f = []\n",
    "\n",
    "time_f.append('uncompleted_date')\n",
    "\n",
    "x_data['uncompleted_date'] = x_data[date_col].apply(lambda x: len(x[x==-1])*1.0/len(date_col),axis=1)\n",
    "x_data_t['uncompleted_date'] = x_data_t[date_col].apply(lambda x: len(x[x==-1])*1.0/len(date_col),axis=1)\n",
    "\n",
    "for i in range(len(date_col)):\n",
    "    for j in range(i+1,len(date_col)):\n",
    "        var_f = 'delta_time_'+str(i)+'_'+str(j)\n",
    "        time_f.append(var_f)\n",
    "        x_data[var_f] = x_data[date_col[j]] - x_data[date_col[i]] \n",
    "        x_data_t[var_f] = x_data_t[date_col[j]] - x_data_t[date_col[i]] \n",
    "        \n",
    "print len(time_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "is both state the same ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_data['varstate'] = x_data[state_name].apply(lambda x: 1 if x[state_name[0]] == x[state_name[1]] else 0,axis =1)\n",
    "x_data_t['varstate'] = x_data_t[state_name].apply(lambda x: 1 if x[state_name[0]] == x[state_name[1]] else 0,axis =1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count error code to understand influence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "#create some new indicators\n",
    "\n",
    "max_value = {}\n",
    "\n",
    "mod_col = list(set(int_data + float_data).difference(dups + out_data))\n",
    "\n",
    "for col in mod_col:\n",
    "    max_value[col] = x_data[col].max()\n",
    "    \n",
    "review_col = []\n",
    "for col in mod_col:\n",
    "    if (max_value[col] + 1) == (10**len(str(max_value[col]))) and len(str(max_value[col])) > 1:\n",
    "        review_col.append(col)\n",
    "review_col = np.array(review_col)\n",
    "\n",
    "max_list = []\n",
    "for col in review_col:\n",
    "    max_list.append(max_value[col])\n",
    "max_list = np.array(max_list)        \n",
    "        \n",
    "mod = ['count_99','count_98','count_97','count_96']\n",
    "\n",
    "def count_errors(line):\n",
    "    res = line.copy()\n",
    "    vals = line[review_col].values\n",
    "    for i in range(4):\n",
    "        res[mod[i]] = max_list[vals == (max_list - i)].shape[0]\n",
    "        #res[review_col[vals == (max_list - i)]] = -1\n",
    "    return res\n",
    "          \n",
    "for col in mod: \n",
    "    x_data[col] = np.zeros(x_data.shape[0])\n",
    "    x_data_t[col] = np.zeros(x_data_t.shape[0])\n",
    "\n",
    "cuts = np.r_[np.arange(0,145231,10000),[145231]]\n",
    "for i in range(15):\n",
    "    x_data.ix[cuts[i]:cuts[i+1]-1,:] = x_data.ix[cuts[i]:cuts[i+1]-1,:].apply(lambda x : count_errors(x),axis=1)\n",
    "    print i\n",
    "    \n",
    "cuts = np.r_[np.arange(0,145232,10000),[145232]]\n",
    "for i in range(15):\n",
    "    x_data_t.ix[cuts[i]:cuts[i+1]-1,:] = x_data_t.ix[cuts[i]:cuts[i+1]-1,:].apply(lambda x : count_errors(x),axis=1)\n",
    "    print i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge the features and save them for the next step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols = list(set(x_data.columns).difference(categorical_data + city_name + employement_data))\n",
    "\n",
    "x_train = np.concatenate((x_data[cols],x_cat,dfs[0].todense(),dfs[1].todense(),dfsc[0].todense()),axis=1).astype('int32')\n",
    "x_test = np.concatenate((x_data_t[cols],x_cat_t,dfst[0].todense(),dfst[1].todense(),dfsct[0].todense()),axis=1).astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savetxt(\"Features/train1309\",x_train)\n",
    "np.savetxt(\"Features/ytrain1309\",y_train)\n",
    "np.savetxt(\"Features/test1309\",x_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
