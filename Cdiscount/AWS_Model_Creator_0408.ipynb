{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import unicodedata as uni\n",
    "import time\n",
    "import re\n",
    "def normalize(string):\n",
    "    return uni.normalize('NFKD',string).encode('ascii','ignore')\n",
    "from sys import stdout\n",
    "import scipy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF with IDF on categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TRAINING_SET_PATH = 'Data/cleantrain.csv'\n",
    "TEST_SET_PATH = 'Data/cleantest.csv'\n",
    "RAYON_SET_PATH = 'Data/rayon.csv'\n",
    "SEPARATOR = ','\n",
    "INDEX_COL = 'Identifiant_Produit'\n",
    "PRICE_COL = 'prix'\n",
    "CAT_COL1 = 'Categorie1'\n",
    "CAT_COL2 = 'Categorie2'\n",
    "CAT_COL3 = 'Categorie3'\n",
    "DESC_COL = 'Description'\n",
    "LIBE_COL = 'Libelle'\n",
    "MARQ_COL = 'Marque'\n",
    "PRICE_COL = 'prix'\n",
    "CDIS_COL = 'Produit_Cdiscount'\n",
    "MARQ_DESC = 'Marque_in_Description' \n",
    "MARQ_LIBE = 'Marque_in_Libelle'\n",
    "INT_LIBE_DESC = 'Intersection_Libe_Desc'\n",
    "LEN_LIBE = 'Longueur_Libelle'\n",
    "LEN_DESC = 'Longueur_Description'\n",
    "LEN_INT = 'Longueur Intersection'\n",
    "LIBE_DESC = 'Percent_Libe_in_Desc'\n",
    "PERCENT_CAPTURE_LIBE = 'Percent_capture_libe'\n",
    "PERCENT_CAPTURE_DESC = 'Percent_capture_desc'\n",
    "PERCENT_CAPTURE_INT = 'Percent_capture_int'\n",
    "\n",
    "AUGMENTED_DESC_COL = 'Description+Libelle'\n",
    "\n",
    "VOCABULARY_PATH_DESC = 'Data/vocabularyDesc.npy'\n",
    "VOCABULARY_PATH_LIBE = 'Data/vocabularyLibe.npy'\n",
    "VOCABULARY_PATH_INT = 'Data/vocabularyInt.npy'\n",
    "\n",
    "PRED_PATH_LOG = 'Prediction/Pred_log_Cdiscount'\n",
    "STOPLIST = set('un une le les la au aux de des ce se ma mon ton sa the of then it ou and tres sur on en in dans pour for with avec et ne pas tout all without sans du il votre son one je vous nos ses vos est tu es cet cette tout toute'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"loading Data\"\n",
    "Ctrain = pd.read_csv(TRAINING_SET_PATH, sep=SEPARATOR,index_col=INDEX_COL)\n",
    "Ctrain.fillna(' ',inplace=True)\n",
    "Ctest = pd.read_csv(TEST_SET_PATH, sep=SEPARATOR,index_col=INDEX_COL)\n",
    "Ctest.fillna(' ',inplace=True)\n",
    "vocabularyDesc = np.load(VOCABULARY_PATH_DESC)\n",
    "vocabularyLibe = np.load(VOCABULARY_PATH_LIBE)\n",
    "vocabularyInt = np.load(VOCABULARY_PATH_INT)\n",
    "vocDesc = set(vocabularyDesc)\n",
    "vocLibe = set(vocabularyLibe)\n",
    "vocInt = set(vocabularyInt)\n",
    "#voc2 = voc.difference(STOPLIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Create a well distributed training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_training_set(data_y,min_sample_by_cat,max_sample_by_cat):\n",
    "    y = np.array(data_y)\n",
    "    y_unique = data_y.unique()\n",
    "    all_indices = np.arange(0,y.shape[0],1).astype(int)\n",
    "    indices_by_cat = np.array([])\n",
    "    y_indices = np.array([])\n",
    "    progress = 0\n",
    "    length = len(y_unique)\n",
    "    for cat in y_unique:\n",
    "        cat_indices = all_indices[y==cat]\n",
    "        len_cat = len(cat_indices)\n",
    "        if len_cat < min_sample_by_cat:\n",
    "            indices_by_cat = np.r_[indices_by_cat,np.random.choice(cat_indices,min_sample_by_cat,replace=True)]\n",
    "            y_indices =  np.r_[y_indices,[cat]*min_sample_by_cat]\n",
    "        elif len_cat < max_sample_by_cat:\n",
    "            indices_by_cat = np.r_[indices_by_cat,np.random.choice(cat_indices,len_cat,replace=True)]\n",
    "            y_indices =  np.r_[y_indices,[cat]*len_cat]\n",
    "        else:\n",
    "            indices_by_cat = np.r_[indices_by_cat,np.random.choice(cat_indices,max_sample_by_cat,replace=True)]\n",
    "            y_indices =  np.r_[y_indices,[cat]*max_sample_by_cat]\n",
    "        progress +=1\n",
    "        percent = progress * 100.0/ length\n",
    "        count = int(np.floor(percent/2))\n",
    "        stdout.write(\"\\r\\x1b[K\" + \"[\" + count * \"-\" + (50-count)*\" \" + \"] %.1f\" %percent)\n",
    "        stdout.flush()\n",
    "    set_indices = np.array(indices_by_cat).flatten()\n",
    "    return set_indices,y_indices\n",
    "\n",
    "def create_training_set(data_y,min_sample_by_cat,max_sample_by_cat):\n",
    "    y = np.array(data_y)\n",
    "    y_unique = data_y.unique()\n",
    "    all_indices = np.arange(0,y.shape[0],1).astype(int)\n",
    "    indices_by_cat = np.array([])\n",
    "    progress = 0\n",
    "    length = len(y_unique)\n",
    "    t = time.time()\n",
    "    for cat in y_unique:\n",
    "        cat_indices = all_indices[y==cat]\n",
    "        len_cat = len(cat_indices)\n",
    "        if len_cat < min_sample_by_cat:\n",
    "            indices_by_cat = np.r_[indices_by_cat,np.random.choice(cat_indices,min_sample_by_cat,replace=True)]\n",
    "        elif len_cat < max_sample_by_cat:\n",
    "            indices_by_cat = np.r_[indices_by_cat,np.random.choice(cat_indices,len_cat,replace=True)]\n",
    "        else:\n",
    "            indices_by_cat = np.r_[indices_by_cat,np.random.choice(cat_indices,max_sample_by_cat,replace=True)]\n",
    "        progress +=1\n",
    "        percent = progress * 100.0/ length\n",
    "        count = int(np.floor(percent/2))\n",
    "        elapse_t = time.time() - t\n",
    "        eta = elapse_t * (100.0 - percent) / percent / 60\n",
    "        stdout.write(\"\\r\\x1b[K\" + \"[\" + count * \"-\" + (50-count)*\" \" + \"] %.1f p\" %percent + \"       eta :  %.1f min\" %eta )\n",
    "        stdout.flush()\n",
    "    set_indices = np.array(indices_by_cat).flatten()\n",
    "    return set_indices\n",
    "\n",
    "def create_indicies_ref_by_cat(data_y):\n",
    "    y = np.array(data_y)\n",
    "    y_unique = data_y.unique()\n",
    "    res = {}\n",
    "    all_indices = np.arange(0,y.shape[0],1).astype(int)\n",
    "    progress = 0\n",
    "    length = len(y_unique)\n",
    "    t = time.time()\n",
    "    for cat in y_unique:\n",
    "        cat_indices = all_indices[y==cat]\n",
    "        res[cat] = cat_indices\n",
    "        progress +=1\n",
    "        percent = progress * 100.0/ length\n",
    "        count = int(np.floor(percent/2))\n",
    "        elapse_t = time.time() - t\n",
    "        eta = elapse_t * (100.0 - percent) / percent / 60\n",
    "        stdout.write(\"\\r\\x1b[K\" + \"[\" + count * \"-\" + (50-count)*\" \" + \"] %.1f p\" %percent + \"       eta :  %.1f min\" %eta )\n",
    "        stdout.flush()\n",
    "    return res\n",
    "\n",
    "def create_training_set_with_all_marque(data_y,data_marques,min_sample_by_cat,max_sample_by_cat,min_by_marque,cat_indices_dic):\n",
    "    y = np.array(data_y)\n",
    "    marques = np.array(data_marques)\n",
    "    y_unique = data_y.unique()\n",
    "    all_indices = np.arange(0,y.shape[0],1).astype(int)\n",
    "    indices_by_cat = np.array([])\n",
    "    progress = 0\n",
    "    length = len(y_unique)\n",
    "    t = time.time()\n",
    "    for cat in y_unique:\n",
    "        cat_indices = cat_indices_dic[cat]\n",
    "        marques_cat = marques[cat_indices]\n",
    "        marques_cat_unique = set(marques_cat)\n",
    "        len_cat = len(cat_indices)\n",
    "        if len_cat < min_sample_by_cat:\n",
    "            indices_by_cat = np.r_[indices_by_cat,np.random.choice(cat_indices,min_sample_by_cat,replace=True)]\n",
    "        elif len_cat < max_sample_by_cat:\n",
    "            indices_by_cat = np.r_[indices_by_cat,np.random.choice(cat_indices,len_cat,replace=True)]\n",
    "        elif len(marques_cat_unique) < max_sample_by_cat:\n",
    "            for marque in set(marques_cat):\n",
    "                cat_marque_indices = cat_indices[marques_cat == marque]\n",
    "                indices_by_cat = np.r_[indices_by_cat,np.random.choice(cat_marque_indices,min_by_marque,replace=True)]\n",
    "            indices_by_cat = np.r_[indices_by_cat,np.random.choice(cat_indices,max_sample_by_cat-len(marques_cat_unique),replace=True)]\n",
    "        else:\n",
    "            for marque in set(marques_cat):\n",
    "                cat_marque_indices = cat_indices[marques_cat == marque]\n",
    "                indices_by_cat = np.r_[indices_by_cat,np.random.choice(cat_marque_indices,min_by_marque,replace=True)]\n",
    "        progress +=1\n",
    "        percent = progress * 100.0/ length\n",
    "        count = int(np.floor(percent/2))\n",
    "        elapse_t = time.time() - t\n",
    "        eta = elapse_t * (100.0 - percent) / percent / 60\n",
    "        stdout.write(\"\\r\\x1b[K\" + \"[\" + count * \"-\" + (50-count)*\" \" + \"] %.1f p\" %percent + \"       eta :  %.1f min\" %eta )\n",
    "        stdout.flush()\n",
    "    set_indices = np.array(indices_by_cat).flatten()\n",
    "    return set_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K[--------------------------------------------------] 100.0 p       eta :  0.0 min"
     ]
    }
   ],
   "source": [
    "cat_indices_dic = create_indicies_ref_by_cat(Ctrain[CAT_COL3])\n",
    "print \" \"\n",
    "train_ind = create_training_set_with_all_marque(Ctrain[CAT_COL3],Ctrain['Marque'],40,100,2,cat_indices_dic)\n",
    "print \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train creation\n",
      "\u001b[K[--------------------------------------------------] 100.0 p       eta :  0.0 min \n",
      "train size 774650\n",
      "test creation\n",
      "\u001b[K[--------------------------------------------------] 100.0 p       eta :  0.0 min \n",
      "test size 53978\n"
     ]
    }
   ],
   "source": [
    "print \"train creation\"\n",
    "train_ind = create_training_set(Ctrain[CAT_COL3],60,200)\n",
    "print \" \"\n",
    "print \"train size\", train_ind.shape[0]\n",
    "print \"test creation\"\n",
    "test_ind = create_training_set(Ctrain[CAT_COL3],4,10)\n",
    "print \" \"\n",
    "print \"test size\", test_ind.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Create a model to find probabilities of categories 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating Tfidf for categorie 1\n",
      "creating Tfidf for categorie 1\n",
      "creating Count for categorie 1\n",
      "creating Count for categorie 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer=u'word', binary=True, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "print \"creating Tfidf for categorie 1\"\n",
    "Tfidf1Desc = TfidfVectorizer(vocabulary=vocDesc)\n",
    "CategorieDOC = Ctrain.ix[:,[CAT_COL1,DESC_COL]].groupby(CAT_COL1)[DESC_COL].apply(lambda x: \"%s\" % ' '.join(x))\n",
    "Libelle = CategorieDOC.reset_index()[DESC_COL]\n",
    "cat1Desc_vector = Tfidf1Desc.fit_transform(Libelle)\n",
    "del CategorieDOC\n",
    "del Libelle\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "print \"creating Tfidf for categorie 1\"\n",
    "Tfidf1Libe = TfidfVectorizer(vocabulary=vocLibe)\n",
    "CategorieDOC = Ctrain.ix[:,[CAT_COL1,LIBE_COL]].groupby(CAT_COL1)[LIBE_COL].apply(lambda x: \"%s\" % ' '.join(x))\n",
    "Libelle = CategorieDOC.reset_index()[LIBE_COL]\n",
    "cat1Libe_vector = Tfidf1Libe.fit_transform(Libelle)\n",
    "del CategorieDOC\n",
    "del Libelle\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "print \"creating Count for categorie 1\"\n",
    "Count1Int = CountVectorizer(vocabulary=vocInt)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "print \"creating Count for categorie 1\"\n",
    "Count1Marq = CountVectorizer(binary=True)\n",
    "Count1Marq.fit(Ctrain[MARQ_COL])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelBinarizer(neg_label=0, pos_label=1, sparse_output=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "y_cat1_number = preprocessing.LabelEncoder()\n",
    "y_cat1_table = preprocessing.LabelBinarizer()\n",
    "y_cat1_number.fit(Ctrain[CAT_COL1].unique())\n",
    "y_cat1_table.fit(y_cat1_number.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<774650x252344 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 17933648 stored elements in COOrdinate format>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train1 = y_cat1_number.transform(Ctrain.ix[train_ind,CAT_COL1])\n",
    "trainVector1Desc = Tfidf1Desc.transform(Ctrain.ix[train_ind,DESC_COL])\n",
    "trainVector1Libe = Tfidf1Libe.transform(Ctrain.ix[train_ind,LIBE_COL])\n",
    "trainVector1Int = Count1Int.transform(Ctrain.ix[train_ind,INT_LIBE_DESC])\n",
    "trainVector1Marq = Count1Marq.transform(Ctrain.ix[train_ind,MARQ_COL])\n",
    "price_mat = np.array(Ctrain.ix[train_ind,PRICE_COL].apply(lambda x: np.log10(x+2)))\n",
    "features_mat = Ctrain.ix[train_ind,[LEN_LIBE,LEN_DESC,LEN_INT,LIBE_DESC,MARQ_DESC,MARQ_LIBE]]\n",
    "#cat1_vector_val = trainVector1.dot(cat1_vector.T)\n",
    "trainVector1 = scipy.sparse.hstack([trainVector1Desc,trainVector1Libe,trainVector1Int,trainVector1Marq,price_mat.reshape((price_mat.shape[0],1)),features_mat])\n",
    "trainVector1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<53978x252344 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 1271448 stored elements in COOrdinate format>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test1 = y_cat1_number.transform(Ctrain.ix[test_ind,CAT_COL1])\n",
    "testVector1Desc = Tfidf1Desc.transform(Ctrain.ix[test_ind,DESC_COL])\n",
    "testVector1Libe = Tfidf1Libe.transform(Ctrain.ix[test_ind,LIBE_COL])\n",
    "testVector1Int = Count1Int.transform(Ctrain.ix[test_ind,INT_LIBE_DESC])\n",
    "testVector1Marq = Count1Marq.transform(Ctrain.ix[test_ind,MARQ_COL])\n",
    "price_mat = np.array(Ctrain.ix[test_ind,PRICE_COL].apply(lambda x: np.log10(x+2)))\n",
    "features_mat = Ctrain.ix[test_ind,[LEN_LIBE,LEN_DESC,LEN_INT,LIBE_DESC,MARQ_DESC,MARQ_LIBE]]\n",
    "#cat1_vector_val = trainVector1.dot(cat1_vector.T)\n",
    "testVector1 = scipy.sparse.hstack([testVector1Desc,testVector1Libe,testVector1Int,testVector1Marq,price_mat.reshape((price_mat.shape[0],1)),features_mat])\n",
    "testVector1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]93\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "mod1 = LogisticRegression(verbose=5,max_iter=100)\n",
    "mod1.fit(trainVector1,y_train1)\n",
    "val = int(np.floor(mod1.score(testVector1,y_test1)*100))\n",
    "print val\n",
    "predict1 = mod1.predict_proba(testVector1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "mod = RandomForestClassifier(n_jobs=16, n_estimators=60)\n",
    "mod.fit(trainVector1,y_train1)\n",
    "val = int(np.floor(mod.score(testVector1,y_test1)*100))\n",
    "print val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95.6551991222\n"
     ]
    }
   ],
   "source": [
    "print mod1.score(trainVector1,y_train1)*100\n",
    "y_pred = mod1.predict(testVector1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 141    0    0 ...,    0    0    0]\n",
      " [   0  174    0 ...,    0    0    0]\n",
      " [   0    0  329 ...,    1    0    0]\n",
      " ..., \n",
      " [   0    0    0 ..., 1517    0    1]\n",
      " [   0    0    0 ...,    0  150    0]\n",
      " [   0    0    0 ...,    0    0  989]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f1712d00fd0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP0AAAD7CAYAAAChbJLhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nA4I0nkjaGGLRAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f171341ec10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test1, y_pred)\n",
    "print(cm)\n",
    "plt.matshow(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Create a model for categorie 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating Tfidf for categorie 2\n",
      "creating Tfidf for categorie 2\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "print \"creating Tfidf for categorie 2\"\n",
    "Tfidf2Desc = TfidfVectorizer(vocabulary=vocDesc)\n",
    "CategorieDOC = Ctrain.ix[:,[CAT_COL2,DESC_COL]].groupby(CAT_COL2)[DESC_COL].apply(lambda x: \"%s\" % ' '.join(x))\n",
    "Libelle = CategorieDOC.reset_index()[DESC_COL]\n",
    "cat2Desc_vector = Tfidf2Desc.fit_transform(Libelle)\n",
    "del CategorieDOC\n",
    "del Libelle\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "print \"creating Tfidf for categorie 2\"\n",
    "Tfidf2Libe = TfidfVectorizer(vocabulary=vocLibe)\n",
    "CategorieDOC = Ctrain.ix[:,[CAT_COL2,LIBE_COL]].groupby(CAT_COL2)[LIBE_COL].apply(lambda x: \"%s\" % ' '.join(x))\n",
    "Libelle = CategorieDOC.reset_index()[LIBE_COL]\n",
    "cat2Libe_vector = Tfidf2Libe.fit_transform(Libelle)\n",
    "del CategorieDOC\n",
    "del Libelle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "536\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "y_cat2_number = preprocessing.LabelEncoder()\n",
    "y_cat2_table = preprocessing.LabelBinarizer()\n",
    "y_cat2_number.fit(Ctrain[CAT_COL2].unique())\n",
    "y_cat2_table.fit(y_cat2_number.classes_)\n",
    "print len(y_cat2_number.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<774650x252396 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 17933648 stored elements in COOrdinate format>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train2 = y_cat2_number.transform(Ctrain.ix[train_ind,CAT_COL2])\n",
    "trainVector2Desc = Tfidf2Desc.transform(Ctrain.ix[train_ind,DESC_COL])\n",
    "trainVector2Libe = Tfidf2Libe.transform(Ctrain.ix[train_ind,LIBE_COL])\n",
    "trainVector1Int = Count1Int.transform(Ctrain.ix[train_ind,INT_LIBE_DESC])\n",
    "trainVector1Marq = Count1Marq.transform(Ctrain.ix[train_ind,MARQ_COL])\n",
    "price_mat = np.array(Ctrain.ix[train_ind,PRICE_COL].apply(lambda x: np.log10(x+2)))\n",
    "features_mat = Ctrain.ix[train_ind,[LEN_LIBE,LEN_DESC,LEN_INT,LIBE_DESC,MARQ_DESC,MARQ_LIBE]]\n",
    "cat1_pred = y_cat1_table.transform(y_train1)\n",
    "trainVector2 = scipy.sparse.hstack([trainVector2Desc,trainVector2Libe,trainVector1Int,trainVector1Marq,price_mat.reshape((price_mat.shape[0],1)),features_mat])\n",
    "trainVector2 = scipy.sparse.hstack([trainVector2,cat1_pred])\n",
    "trainVector2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<53978x252396 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 4078304 stored elements in COOrdinate format>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test2 = y_cat2_number.transform(Ctrain.ix[test_ind,CAT_COL2])\n",
    "testVector2Desc = Tfidf2Desc.transform(Ctrain.ix[test_ind,DESC_COL])\n",
    "testVector2Libe = Tfidf2Libe.transform(Ctrain.ix[test_ind,LIBE_COL])\n",
    "testVector1Int = Count1Int.transform(Ctrain.ix[test_ind,INT_LIBE_DESC])\n",
    "testVector1Marq = Count1Marq.transform(Ctrain.ix[test_ind,MARQ_COL])\n",
    "price_mat = np.array(Ctrain.ix[test_ind,PRICE_COL].apply(lambda x: np.log10(x+2)))\n",
    "features_mat = Ctrain.ix[test_ind,[LEN_LIBE,LEN_DESC,LEN_INT,LIBE_DESC,MARQ_DESC,MARQ_LIBE]]\n",
    "cat1_pred = mod1.predict_proba(testVector1)\n",
    "testVector2 = scipy.sparse.hstack([testVector2Desc,testVector2Libe,testVector1Int,testVector1Marq,price_mat.reshape((price_mat.shape[0],1)),features_mat])\n",
    "testVector2 = scipy.sparse.hstack([testVector2,cat1_pred])\n",
    "testVector2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "mod2 = LogisticRegression(verbose=5,max_iter=100)\n",
    "mod2.fit(trainVector2,y_train2)\n",
    "val = int(np.floor(mod2.score(testVector2,y_test2)*100))\n",
    "print val\n",
    "predict2 = mod2.predict_proba(testVector2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rayon = pd.read_csv(RAYON_SET_PATH,sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ray = rayon[['Categorie1','Categorie2']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_parent_cat(cat2):\n",
    "    return ray[ray['Categorie2'] == cat2]['Categorie1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def recalculate_proba(pred_proba2,pred_proba1):\n",
    "    proba = np.zeros(pred_proba2.shape)\n",
    "    for i in range(pred_proba2.shape[1]):\n",
    "        j = y_cat1_number.transform(find_parent_cat(y_cat2_number.inverse_transform(i)))[0]\n",
    "        proba[:,i] = pred_proba2[:,i] * pred_proba1[:,j]\n",
    "    return proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_cat2_number.inverse_transform(0)\n",
    "predictn = recalculate_proba(predict2,predict1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predn = np.argmax(predictn,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.891826299604\n"
     ]
    }
   ],
   "source": [
    "res = 0\n",
    "for i in range(len(y_test2)):\n",
    "    if predn[i] == y_test2[i]:\n",
    "        res +=1\n",
    "print res * 1.0 / len(y_test2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "categorie3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating Tfidf for categorie 3\n",
      "creating Tfidf for categorie 3\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "print \"creating Tfidf for categorie 3\"\n",
    "Tfidf3Desc = TfidfVectorizer(vocabulary=vocDesc)\n",
    "CategorieDOC = Ctrain.ix[:,[CAT_COL3,DESC_COL]].groupby(CAT_COL3)[DESC_COL].apply(lambda x: \"%s\" % ' '.join(x))\n",
    "Libelle = CategorieDOC.reset_index()[DESC_COL]\n",
    "cat3Desc_vector = Tfidf3Desc.fit_transform(Libelle)\n",
    "del CategorieDOC\n",
    "del Libelle\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "print \"creating Tfidf for categorie 3\"\n",
    "Tfidf3Libe = TfidfVectorizer(vocabulary=vocLibe)\n",
    "CategorieDOC = Ctrain.ix[:,[CAT_COL3,LIBE_COL]].groupby(CAT_COL3)[LIBE_COL].apply(lambda x: \"%s\" % ' '.join(x))\n",
    "Libelle = CategorieDOC.reset_index()[LIBE_COL]\n",
    "cat3Libe_vector = Tfidf3Libe.fit_transform(Libelle)\n",
    "del CategorieDOC\n",
    "del Libelle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5789\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "y_cat3_number = preprocessing.LabelEncoder()\n",
    "y_cat3_table = preprocessing.LabelBinarizer()\n",
    "y_cat3_number.fit(Ctrain[CAT_COL3].unique())\n",
    "y_cat3_table.fit(y_cat3_number.classes_)\n",
    "print len(y_cat3_number.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<774650x252932 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 17936537 stored elements in COOrdinate format>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train3 = y_cat3_number.transform(Ctrain.ix[train_ind,CAT_COL3])\n",
    "trainVector3Desc = Tfidf3Desc.transform(Ctrain.ix[train_ind,DESC_COL])\n",
    "trainVector3Libe = Tfidf3Libe.transform(Ctrain.ix[train_ind,LIBE_COL])\n",
    "trainVector1Int = Count1Int.transform(Ctrain.ix[train_ind,INT_LIBE_DESC])\n",
    "trainVector1Marq = Count1Marq.transform(Ctrain.ix[train_ind,MARQ_COL])\n",
    "price_mat = np.array(Ctrain.ix[train_ind,PRICE_COL].apply(lambda x: np.log10(x+2)))\n",
    "features_mat = Ctrain.ix[train_ind,[LEN_LIBE,LEN_DESC,LEN_INT,LIBE_DESC,MARQ_DESC,MARQ_LIBE]]\n",
    "cat1_pred = y_cat1_table.transform(y_train1)\n",
    "cat2_pred = y_cat2_table.transform(y_train2)\n",
    "trainVector3 = scipy.sparse.hstack([trainVector3Desc,trainVector3Libe,trainVector1Int,trainVector1Marq,price_mat.reshape((price_mat.shape[0],1)),features_mat])\n",
    "trainVector3 = scipy.sparse.hstack([trainVector3,cat1_pred,cat2_pred])\n",
    "trainVector3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<53978x252932 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 33010512 stored elements in COOrdinate format>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test3 = y_cat3_number.transform(Ctrain.ix[test_ind,CAT_COL3])\n",
    "testVector3Desc = Tfidf3Desc.transform(Ctrain.ix[test_ind,DESC_COL])\n",
    "testVector3Libe = Tfidf3Libe.transform(Ctrain.ix[test_ind,LIBE_COL])\n",
    "testVector1Int = Count1Int.transform(Ctrain.ix[test_ind,INT_LIBE_DESC])\n",
    "testVector1Marq = Count1Marq.transform(Ctrain.ix[test_ind,MARQ_COL])\n",
    "price_mat = np.array(Ctrain.ix[test_ind,PRICE_COL].apply(lambda x: np.log10(x+2)))\n",
    "features_mat = Ctrain.ix[test_ind,[LEN_LIBE,LEN_DESC,LEN_INT,LIBE_DESC,MARQ_DESC,MARQ_LIBE]]\n",
    "cat1_pred = mod1.predict_proba(testVector1)\n",
    "cat2_pred = mod2.predict_proba(testVector2)\n",
    "testVector3 = scipy.sparse.hstack([testVector3Desc,testVector3Libe,testVector1Int,testVector1Marq,price_mat.reshape((price_mat.shape[0],1)),features_mat])\n",
    "testVector3 = scipy.sparse.hstack([testVector3,cat1_pred,cat2_pred])\n",
    "testVector3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "mod3 = RandomForestClassifier(n_jobs=8, n_estimators=50)\n",
    "mod3.fit(trainVector3,y_train3)\n",
    "val = int(np.floor(mod3.score(testVector3,y_test3)*100))\n",
    "print val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "mod3 = LogisticRegression(verbose=5,max_iter=100)\n",
    "mod3.fit(trainVector3,y_train3)\n",
    "val = int(np.floor(mod3.score(testVector3,y_test3)*100))\n",
    "print val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "mod4 = OneVsRestClassifier(LinearSVC(),n_jobs=4)\n",
    "mod4.fit(trainVector3,y_train3)\n",
    "val = int(np.floor(mod4.score(testVector3,y_test3)*100))\n",
    "print val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorsTest = Tfidf3.transform(Ctest[AUGMENTED_DESC_COL])\n",
    "price_mat = np.array(Ctest[PRICE_COL].apply(lambda x: np.log10(x+2)))\n",
    "vectorsTest = scipy.sparse.hstack([vectorsTest,price_mat.reshape((price_mat.shape[0],1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prediction = Ctest.copy()\n",
    "prediction['Id_Categorie'] = y_cat3_number.inverse_transform(mod4.predict(vectorsTest))\n",
    "prediction.index.name = 'Id_Produit'\n",
    "prediction.drop([PRICE_COL,AUGMENTED_DESC_COL,PERCENT_CAPTURE,'Marque'],inplace=True,axis=1)\n",
    "prediction.to_csv(PRED_PATH_LOG + \"_\" + str(val) + \"_p\" + \".csv\",sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85479269331950058"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod4.score(testVector3,y_test3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
