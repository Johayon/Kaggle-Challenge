{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Module de creation de vocabulaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import unicodedata as uni\n",
    "import re\n",
    "import time\n",
    "def normalize(string):\n",
    "    return uni.normalize('NFKD',string).encode('ascii','ignore')\n",
    "from sys import stdout\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TRAINING_SET_PATH = 'Data/cleantrainV4.csv'\n",
    "TEST_SET_PATH = 'Data/cleantestV4.csv'\n",
    "SEPARATOR = ','\n",
    "INDEX_COL = 'Identifiant_Produit'\n",
    "\n",
    "CAT_COL = 'Categorie3'\n",
    "DESC_COL = 'Description'\n",
    "LIBE_COL = 'Libelle'\n",
    "MARQ_COL = 'Marque'\n",
    "PRICE_COL = 'Prix'\n",
    "CDIS_COL = 'Produit_Cdiscount'\n",
    "MARQ_DESC = 'Marque_in_Description' \n",
    "MARQ_LIBE = 'Marque_in_Libelle'\n",
    "INT_LIBE_DESC = 'Intersection_Libe_Desc'\n",
    "LEN_LIBE = 'Longueur_Libelle'\n",
    "LEN_DESC = 'Longueur_Description'\n",
    "LEN_INT = 'Longueur Intersection'\n",
    "LIBE_DESC = 'Percent_Libe_in_Desc'\n",
    "\n",
    "PERCENT_CAPTURE_LIBE = 'Percent_capture_libe'\n",
    "PERCENT_CAPTURE_DESC = 'Percent_capture_desc'\n",
    "PERCENT_CAPTURE_INT = 'Percent_capture_int'\n",
    "\n",
    "MIN_DF_BY_CATEGORY_DESC = 0.02\n",
    "MIN_DF_BY_CATEGORY_LIBE = 0.01\n",
    "MIN_DF_BY_CATEGORY_INT = 0.01\n",
    "\n",
    "NGRAM_RANGE_LIBE = (1,1)\n",
    "NGRAM_RANGE_DESC = (1,1)\n",
    "NGRAM_RANGE_INT = (1,1)\n",
    "\n",
    "VOCABULARY_PATH_DESC = 'Data/vocabularyDesc'\n",
    "VOCABULARY_PATH_LIBE = 'Data/vocabularyLibe'\n",
    "VOCABULARY_PATH_INT = 'Data/vocabularyInt'\n",
    "\n",
    "SAVE_TRAINING_PATH = 'Data/cleantrain.csv'\n",
    "SAVE_TEST_PATH = 'Data/cleantest.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Data\n"
     ]
    }
   ],
   "source": [
    "print \"loading Data\"\n",
    "Ctrain = pd.read_csv(TRAINING_SET_PATH, sep=SEPARATOR,index_col=INDEX_COL)\n",
    "Ctrain.fillna(' ',inplace=True)\n",
    "Ctest = pd.read_csv(TEST_SET_PATH, sep=SEPARATOR,index_col=INDEX_COL)\n",
    "Ctest.fillna(' ',inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Price Cleaning try to remove some of the errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get both bigram and single word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print \"get both bigram and single word\"\n",
    "#cuts = np.r_[np.arange(0,15000001,1000000),[15786885]]\n",
    "#for i in range(cuts.shape[0]-1):\n",
    "#    Ctrain.ix[cuts[i]:cuts[i+1]-1,DESC_COL] = Ctrain.ix[cuts[i]:cuts[i+1]-1,DESC_COL].apply(lambda x: \" \".join([word + \" \" + \" \".join(word.split(\"_\"))  if \"_\" in word else word for word in x.split()]))\n",
    "#    stdout.write(\"\\r\\x1b[K\" + \"[\" + (i+1) * \"--\" + (16-(i+1))*\"  \" + \"] %.2f\" %((i+1)*6.25))\n",
    "#    stdout.flush()\n",
    "#Ctest[AUGMENTED_DESC_COL] = Ctest[AUGMENTED_DESC_COL].apply(lambda x: \" \".join([word + \" \" + \" \".join(word.split(\"_\"))  if \"_\" in word else word for word in x.split()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Create a vocabulary by categories by Columns LIBELLE DESCRIPTION MARQUE INT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def create_vocabulary(voc_col,min_df,ngram_range):\n",
    "    Counts = {}\n",
    "    categories = set(Ctrain[CAT_COL].unique())\n",
    "    length = len(categories)\n",
    "    print \"creating vocabulary For \" + voc_col\n",
    "    t = time.time()\n",
    "    progress = 0\n",
    "    for cat in categories:\n",
    "        try:\n",
    "            Counts[cat] = CountVectorizer(min_df=min_df,ngram_range = ngram_range)\n",
    "            subtrain = Ctrain[Ctrain[CAT_COL] == cat]\n",
    "            Counts[cat].fit(subtrain[voc_col])\n",
    "        except:\n",
    "            Counts[cat] = CountVectorizer(min_df=min_df,ngram_range = ngram_range)\n",
    "            Counts[cat].fit(['empty','empty'])\n",
    "        progress +=1\n",
    "        percent = progress * 100.0/ length\n",
    "        count = int(np.floor(percent/2))\n",
    "        elapse_t = time.time() - t\n",
    "        eta = elapse_t * (100.0 - percent) / percent / 60\n",
    "        stdout.write(\"\\r\\x1b[K\" + \"[\" + count * \"-\" + (50-count)*\" \" + \"] %.1f p\" %percent + \"       eta :  %.1f min\" %eta )\n",
    "        stdout.flush()\n",
    "    elapse_t = (time.time() - t) / 60\n",
    "    print \" \"\n",
    "    print \"done in %.1f min\" %elapse_t\n",
    "    \n",
    "    vocabulary = set()\n",
    "    print \"merging vocabulary\"\n",
    "    t = time.time()\n",
    "    progress = 0\n",
    "    for cat in categories:\n",
    "        vocabulary = vocabulary.union(set(Counts[cat].vocabulary_.keys()))\n",
    "        progress +=1\n",
    "        percent = progress * 100.0/ length\n",
    "        count = int(np.floor(percent/2))\n",
    "        elapse_t = time.time() - t\n",
    "        eta = elapse_t * (100.0 - percent) / percent / 60\n",
    "        stdout.write(\"\\r\\x1b[K\" + \"[\" + count * \"-\" + (50-count)*\" \" + \"] %.1f p\" %percent + \"       eta :  %.1f min\" %eta )\n",
    "        stdout.flush()\n",
    "    elapse_t = (time.time() - t) / 60\n",
    "    print \" \"\n",
    "    print \"done in %.1f min\" % elapse_t\n",
    "    print \"vocabulary length %d words\" % len(vocabulary)\n",
    "    return vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating vocabulary For Description\n",
      "\u001b[K[--------------------------------------------------] 100.0 p       eta :  0.0 min \n",
      "done in 26.7 min\n",
      "merging vocabulary\n",
      "\u001b[K[--------------------------------------------------] 100.0 p       eta :  0.0 min \n",
      "done in 0.3 min\n",
      "vocabulary length 91848 words\n"
     ]
    }
   ],
   "source": [
    "vocabularyDESC = create_vocabulary(DESC_COL,MIN_DF_BY_CATEGORY_DESC,NGRAM_RANGE_DESC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating vocabulary For Libelle\n",
      "\u001b[K[--------------------------------------------------] 100.0 p       eta :  0.0 min \n",
      "done in 25.5 min\n",
      "merging vocabulary\n",
      "\u001b[K[--------------------------------------------------] 100.0 p       eta :  0.0 min \n",
      "done in 0.2 min\n",
      "vocabulary length 76166 words\n"
     ]
    }
   ],
   "source": [
    "vocabularyLIBE = create_vocabulary(LIBE_COL,MIN_DF_BY_CATEGORY_LIBE,NGRAM_RANGE_LIBE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating vocabulary For Intersection_Libe_Desc\n",
      "\u001b[K[--------------------------------------------------] 100.0 p       eta :  0.0 min \n",
      "done in 24.9 min\n",
      "merging vocabulary\n",
      "\u001b[K[--------------------------------------------------] 100.0 p       eta :  0.0 min \n",
      "done in 0.2 min\n",
      "vocabulary length 56872 words\n"
     ]
    }
   ],
   "source": [
    "vocabularyINT = create_vocabulary(INT_LIBE_DESC,MIN_DF_BY_CATEGORY_INT,NGRAM_RANGE_INT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count word and compute word catching by document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def insert_percent(voc,colin,colout):\n",
    "    newcount = CountVectorizer(vocabulary=voc)\n",
    "    vect = newcount.transform(Ctrain[colin])\n",
    "    s = vect.sum(axis=1).flatten()\n",
    "    Ctrain['temp'] = s.T\n",
    "    cuts = np.r_[np.arange(0,15000001,1000000),[15786885]]\n",
    "    for i in range(cuts.shape[0]-1):\n",
    "        Ctrain.ix[cuts[i]:cuts[i+1]-1,colout] = Ctrain.ix[cuts[i]:cuts[i+1]-1,[colin,'temp']].apply(lambda x: (x['temp']+1)*1.0/(len(x[colin].split())+1),axis=1)\n",
    "        stdout.write(\"\\r\\x1b[K\" + \"[\" + (i+1) * \"--\" + (16-(i+1))*\"  \" + \"] %.2f\" %((i+1)*6.25))\n",
    "        stdout.flush()\n",
    "    Ctrain.drop('temp',axis=1,inplace=True)\n",
    "    vect2 = newcount.transform(Ctest[colin])\n",
    "    s2 = vect2.sum(axis=1).flatten()\n",
    "    Ctest['temp'] = s2.T\n",
    "    Ctest[colout] = Ctest[[colin,'temp']].apply(lambda x: (x['temp']+1)*1.0/(len(x[colin].split())+1),axis=1)\n",
    "    Ctest.drop('temp',axis=1,inplace=True)\n",
    "    print ' '\n",
    "    print 'Done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Description percent\n",
      "\u001b[K[--------------------------------] 100.00 \n",
      "Done\n",
      "Libelle percent\n",
      "\u001b[K[--------------------------------] 100.00 \n",
      "Done\n",
      "intersection percent\n",
      "\u001b[K[--------------------------------] 100.00 \n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "print \"Description percent\"\n",
    "insert_percent(vocabularyDESC,DESC_COL,PERCENT_CAPTURE_DESC)\n",
    "print \"Libelle percent\"\n",
    "insert_percent(vocabularyLIBE,LIBE_COL,PERCENT_CAPTURE_LIBE)\n",
    "print \"Intersection percent\"\n",
    "insert_percent(vocabularyINT,INT_LIBE_DESC,PERCENT_CAPTURE_INT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save results for next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save(VOCABULARY_PATH_DESC,np.array(list(vocabularyDESC)))\n",
    "np.save(VOCABULARY_PATH_LIBE,np.array(list(vocabularyLIBE)))\n",
    "np.save(VOCABULARY_PATH_INT ,np.array(list(vocabularyINT)))\n",
    "\n",
    "Ctrain.to_csv(SAVE_TRAINING_PATH)\n",
    "Ctest.to_csv(SAVE_TEST_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execution time : 1h30 for 1gram\n",
    "\n",
    "Execution time : 2h30 for 1-2gram"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
